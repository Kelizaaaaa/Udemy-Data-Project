{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86ecd2b",
   "metadata": {},
   "source": [
    "# Udemy Scrapping\n",
    "This script aims to pull and scrape data from Udemy regarding the details and information of various courses offered by Udemy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "907e005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library \n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b72d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "#-------------------------------------- Establish connection -----------------------------------#\n",
    "#################################################################################################\n",
    "\n",
    "main_url = \"https://www.udemy.com\"\n",
    "page = requests.get(main_url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "page.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256c7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "#------------------------------------ Extract Subcategories ------------------------------------#\n",
    "#################################################################################################\n",
    "links = soup.find_all(\"a\", class_=\"js-side-nav-cat js-subcat\")\n",
    "\n",
    "# Extract href of subcategories\n",
    "sub_links = []\n",
    "for link in links: \n",
    "    sub_links.append(link[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e8f3f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "#------------------------------------ Extract Course Links -------------------------------------#\n",
    "#################################################################################################\n",
    "# Initialise webdriver\n",
    "delay = 15\n",
    "driver = webdriver.Chrome(\"/Users/kelizaaaaa/Desktop/chromedriver\")\n",
    "course_links = []\n",
    "filters = [\"popularity\", \"highest-rated\", \"newest\"]\n",
    "\n",
    "\n",
    "# Iterate through each subcategory\n",
    "for sub_link in sub_links: \n",
    "    sub_url = main_url + sub_link\n",
    "    \n",
    "    # Loop through 5 pages\n",
    "    for fil in filters:\n",
    "        # Apply filters \n",
    "        for i in range(1, 4):\n",
    "            courses_url = \"{0}?p={1}&sort={2}\".format(sub_url, str(i), fil)\n",
    "            driver.get(courses_url)\n",
    "            try: \n",
    "                WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME, \n",
    "                                                                                   \"course-list--container--3zXPS\")))\n",
    "            except TimeoutException: \n",
    "                print(\"Loading exceeds delay time\")\n",
    "            else: \n",
    "                course_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                course_list = course_soup.find(\"div\", class_=\"course-list--container--3zXPS\")\n",
    "                courses = course_list.find_all(\"a\", class_=\"udlite-custom-focus-visible browse-course-card--link--3KIkQ\")\n",
    "\n",
    "                for course in courses: \n",
    "                    course_links.append(course[\"href\"])\n",
    "\n",
    "            # Sleep\n",
    "            time.sleep(np.random.randint(1,5))\n",
    "\n",
    "    # Free courses\n",
    "    free_url = \"{0}?p=1&price=price-free\".format(sub_url)\n",
    "    driver.get(courses_url)\n",
    "    try: \n",
    "        WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CLASS_NAME, \n",
    "                                                                           \"course-list--container--3zXPS\")))\n",
    "    except TimeoutException: \n",
    "        print(\"Loading exceeds delay time\")\n",
    "    else: \n",
    "        course_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        course_list = course_soup.find(\"div\", class_=\"course-list--container--3zXPS\")\n",
    "        courses = course_list.find_all(\"a\", class_=\"udlite-custom-focus-visible browse-course-card--link--3KIkQ\")\n",
    "\n",
    "        for course in courses: \n",
    "            course_links.append(course[\"href\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "5d7922a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "#------------------------------------ Cleaning Course Links ------------------------------------#\n",
    "#################################################################################################\n",
    "\n",
    "# Remove duplicated courses within course links \n",
    "course_links_unique = list(set(course_links))\n",
    "\n",
    "df_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "86728b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16312\n",
      "16313\n",
      "16314\n",
      "16315\n",
      "16316\n",
      "16317\n",
      "16318\n",
      "16319\n",
      "16320\n",
      "16321\n",
      "16322\n",
      "16323\n",
      "16324\n",
      "16325\n",
      "16326\n",
      "16327\n",
      "16328\n",
      "16329\n",
      "16330\n",
      "16331\n",
      "16332\n",
      "16333\n",
      "16334\n",
      "16335\n",
      "16336\n",
      "16337\n",
      "16338\n",
      "16339\n",
      "16340\n",
      "16341\n",
      "16342\n",
      "16343\n",
      "16344\n",
      "16345\n",
      "16346\n",
      "16347\n",
      "16348\n",
      "16349\n",
      "16350\n",
      "16351\n",
      "16352\n",
      "16353\n",
      "16354\n",
      "16355\n",
      "16356\n",
      "16357\n",
      "16358\n",
      "16359\n",
      "16360\n",
      "16361\n",
      "16362\n",
      "16363\n",
      "16364\n",
      "16365\n",
      "16366\n",
      "16367\n",
      "16368\n",
      "16369\n",
      "16370\n",
      "16371\n",
      "16372\n",
      "16373\n",
      "16374\n",
      "16375\n",
      "16376\n",
      "16377\n",
      "16378\n",
      "16379\n",
      "16380\n",
      "16381\n",
      "16382\n",
      "16383\n",
      "16384\n",
      "16385\n",
      "16386\n",
      "16387\n",
      "16388\n",
      "16389\n",
      "16390\n",
      "16391\n",
      "16392\n",
      "16393\n",
      "16394\n",
      "16395\n",
      "16396\n",
      "16397\n",
      "16398\n",
      "16399\n",
      "16400\n",
      "16401\n",
      "16402\n",
      "16403\n",
      "16404\n",
      "16405\n",
      "16406\n",
      "16407\n",
      "16408\n",
      "16409\n",
      "16410\n",
      "16411\n",
      "16412\n",
      "16413\n",
      "16414\n",
      "16415\n",
      "16416\n",
      "16417\n",
      "16418\n",
      "16419\n",
      "16420\n",
      "16421\n",
      "16422\n",
      "16423\n",
      "16424\n",
      "16425\n",
      "16426\n",
      "16427\n",
      "16428\n",
      "16429\n",
      "16430\n",
      "16431\n",
      "16432\n",
      "16433\n",
      "16434\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#################################################################################################\n",
    "#------------------------------------ Extract Course Details -----------------------------------#\n",
    "#################################################################################################\n",
    "\n",
    "# Iterating through each course link\n",
    "for course_link in course_links_unique:\n",
    "   \n",
    "    # Parse course html\n",
    "    course_url = \"{0}{1}\".format(main_url, course_link)\n",
    "    course_page = requests.get(course_url)\n",
    "    course_soup = BeautifulSoup(course_page.content, \"html.parser\")\n",
    "    course_page.close()\n",
    "    \n",
    "    # Ensuring valid links\n",
    "    if course_soup is None: \n",
    "        continue\n",
    "    else: \n",
    "        \n",
    "        # Extract details part 1\n",
    "        details = json.loads(course_soup.find('script', type='application/ld+json').string.replace(\"\\n\", \"\").strip())\n",
    "\n",
    "        # Title \n",
    "        title = details[0][\"name\"]\n",
    "\n",
    "        # Ratings\n",
    "        try: \n",
    "            overall_rating = details[0][\"aggregateRating\"][\"ratingValue\"]\n",
    "            best_rating = details[0][\"aggregateRating\"][\"bestRating\"]\n",
    "            worst_rating = details[0][\"aggregateRating\"][\"worstRating\"]\n",
    "            num_of_ratings = details[0][\"aggregateRating\"][\"ratingCount\"]\n",
    "        except: \n",
    "            overall_rating = \"0\"\n",
    "            best_rating = \"0\"\n",
    "            worst_rating = \"0\"\n",
    "            num_of_ratings = \"0\"\n",
    "\n",
    "        # Categories\n",
    "        category = details[1][\"itemListElement\"][0][\"name\"]\n",
    "        subcategory = details[1][\"itemListElement\"][1][\"name\"]\n",
    "        try: \n",
    "            topic = details[1][\"itemListElement\"][2][\"name\"]\n",
    "        except: \n",
    "            topic = \"\"\n",
    "\n",
    "        # Instructor \n",
    "        instructor = details[0][\"creator\"][0][\"name\"]\n",
    "\n",
    "        \n",
    "        try: \n",
    "            # Extracting details part 2\n",
    "            details_1 = json.loads(course_soup.find(\"div\", class_=\"ud-component--course-landing-page-udlite--sidebar-container\").\\\n",
    "                                   get(\"data-component-props\"))\n",
    "\n",
    "            # SkillsFuture\n",
    "            skillsfuture = details_1[\"componentProps\"][\"incentives\"][\"is_skills_future\"]\n",
    "\n",
    "            # No. of Practive Tests\n",
    "            num_of_practice_tests = details_1[\"componentProps\"][\"incentives\"][\"num_practice_tests\"]\n",
    "\n",
    "            # No. of articles \n",
    "            num_of_articles = details_1[\"componentProps\"][\"incentives\"][\"num_articles\"]\n",
    "\n",
    "            # No. of coding exercises\n",
    "            num_of_coding_exercises = details_1[\"componentProps\"][\"incentives\"][\"num_coding_exercises\"]\n",
    "\n",
    "            # Video duration \n",
    "            video_duration = details_1[\"componentProps\"][\"incentives\"][\"video_content_length\"]\n",
    "\n",
    "            # No. of additional resources \n",
    "            num_of_additional_resources = details_1[\"componentProps\"][\"incentives\"][\"num_additional_resources\"]\n",
    "        \n",
    "        except: \n",
    "            skillsfuture = \"False\"\n",
    "            num_of_practice_tests = \"0\"\n",
    "            num_of_articles = \"0\"\n",
    "            num_of_coding_exercises = \"0\"\n",
    "            video_duration = np.nan\n",
    "            num_of_additional_resources = \"0\"\n",
    "            \n",
    "            \n",
    "\n",
    "        # Bestseller \n",
    "        bestseller_soup = course_soup.find(\"span\", class_=\"udlite-badge udlite-badge-bestseller udlite-heading-xs\")\n",
    "        if bestseller_soup == None: \n",
    "            bestseller = \"No\"\n",
    "        elif \"Bestseller\" in bestseller_soup.text: \n",
    "            bestseller = \"Yes\"\n",
    "\n",
    "#################################################################################################\n",
    "#------------------------------------- Extract Course Price ------------------------------------#\n",
    "#################################################################################################\n",
    "\n",
    "        # Obtain course ID\n",
    "        course_id = course_soup.find(\"body\").get(\"data-clp-course-id\")\n",
    "        price_api = \"https://www.udemy.com/api-2.0/pricing/?course_ids=\" + course_id + \\\n",
    "        \"&fields[pricing_result]=price,discount_price,list_price,price_detail,price_serve_tracking_id\"\n",
    "        prices_page = requests.get(price_api)\n",
    "        prices = prices_page.json()\n",
    "\n",
    "        price = prices[\"courses\"][course_id][\"list_price\"][\"amount\"]\n",
    "        discounted_price = prices[\"courses\"][course_id][\"discount_price\"]\n",
    "        prices_page.close()\n",
    "\n",
    "#################################################################################################\n",
    "#-------------------------------------- Combining details --------------------------------------#\n",
    "#################################################################################################\n",
    "        course_df = pd.DataFrame([{\n",
    "            \"ID\": course_id, \n",
    "            \"Title\": title, \n",
    "            \"Overall_Rating\": overall_rating, \n",
    "            \"Best_Rating\": best_rating, \n",
    "            \"Worst_Rating\": worst_rating, \n",
    "            \"No_of_Ratings\": num_of_ratings,\n",
    "            \"Category\": category, \n",
    "            \"Subcategory\": subcategory, \n",
    "            \"Topic\": topic, \n",
    "            \"Instructor\": instructor,\n",
    "            \"Language\": language, \n",
    "            \"SkillsFuture\": skillsfuture, \n",
    "            \"No_of_Practice_Test\": num_of_practice_tests, \n",
    "            \"No_of_Articles\": num_of_articles, \n",
    "            \"No_of_Coding_Exercises\": num_of_coding_exercises,\n",
    "            \"Video_Duration\": video_duration, \n",
    "            \"No_of_Additional_Resources\": num_of_additional_resources, \n",
    "            \"Bestseller\": bestseller, \n",
    "            \"Price\": price, \n",
    "            \"Discounted_Price\": discounted_price\n",
    "        }])\n",
    "        df_all = pd.concat([course_df, df_all])\n",
    "        \n",
    "        # Sleep\n",
    "        time.sleep(np.random.randint(1,5))\n",
    "                \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "1cc6615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no duplicated courses within df\n",
    "df_all.drop_duplicates(subset=[\"ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "5e7467ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to raw csv file \n",
    "df_all.to_csv(\"Udemy_Raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69682270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
